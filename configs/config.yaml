experiment_type: translation
summarization:
    mode: "???"
    method: "???"
    experiment_id: "???"
    base_save_dir: "???"

    temperature: 1.0
    buffer_size: 1000
    max_grad_norm: 5.0
    demonstration_ratio: 0.5
    finetune_learning_rate: 0.001
    target_model_sync_steps: 100

    total_steps: null
    task_name: "e2e_nlg_cleaned"
    checkpoint_file_name: null
    save_dir: "${summarization.base_save_dir}/${summarization.temperature}-${summarization.buffer_size}-${summarization.max_grad_norm}-${summarization.demonstration_ratio}-${summarization.finetune_learning_rate}"

translation:
    task_name: "???"
    training_mode: "???"
    architecture: "transformer_small"
    mix_strategy: "mix"
    save_dir: "???"
    # Training time
    num_epochs: 30
    save_frequency: 1
    num_batches_per_epoch: null
    # Paths
    checkpoint_path: null
    base_checkpoint_path: null
    # Other configs
    beam_width: 10
    top_k: null
    top_p: null
    learning_rate: 0.001
    gradient_clipping: true
    # SQL loss
    sql_loss_impl: "v2_v2r_v3_v3r"
    sql_loss_coefficients: null
    sql_loss_margin_constant: null
    sql_loss_margin_coefficient: null
    # Target model
    use_target_network: true
    target_sync_steps: null
    target_sync_method: "polyak"
    target_sql_loss_impl: null
    target_learning_rate: 0.001
    # Rewards
    reward_name: "???"
    reward_shaping: true
    reward_shaping_min: -10
    reward_shaping_max: 10
    # Warmup
    warmup_training_mode: null
    warmup_num_epochs: null
    # Hacks
    hack_truncate_length_constant: null
